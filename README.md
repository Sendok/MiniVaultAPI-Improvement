# ğŸ§  MiniVault API â€“ Full Feature Edition

A fully-local, production-grade REST API that simulates a core feature of ModelVault: receiving a prompt and returning a generated response. Now with true streaming, dynamic model loading, SQLite logging, rate limiting, and Docker support.

---

## ğŸš€ Features

- âœ… Real-time streaming using HuggingFace Transformers (`TextIteratorStreamer`)
- ğŸ” `/generate` for full-text response
- ğŸ“¤ `/generate/stream` for true token-by-token output
- ğŸ”€ Dynamic model loader with automatic caching (`@lru_cache`)
- ğŸ—ƒï¸ SQLite-based prompt-response logging (`db/minivault.db`)
- ğŸ›¡ï¸ Rate limiting via `slowapi` (5 requests/minute per IP)
- ğŸ–¥ï¸ CLI interface using `Typer`
- ğŸ§ª Unit-tested via `pytest`
- ğŸ³ Dockerized for reproducibility

---

## ğŸ› ï¸ Setup Instructions

### 1. Clone and Install

> âš ï¸ Make sure you're using **Python 3.10+**

```bash
git clone https://github.com/Sendok/MiniVaultAPI-Improvement
cd MiniVaultAPI-Improvement
python3 -m venv venv
source venv/bin/activate  # or venv\Scripts\activate on Windows
pip install -r requirements.txt
```

### 2. Run the API Locally

```bash
uvicorn app.main:app --reload
```

Visit the API: [http://localhost:8000](http://localhost:8000)

---

## ğŸ³ Docker Instructions

### Build and Run with Docker:

```bash
docker build -t minivault .
docker run -p 8000:8000 minivault
```

> First run may download `distilgpt2` model via HuggingFace.

---

## ğŸ“¦ API Endpoints

### `POST /generate`

Request:
```json
{ "prompt": "Hello world", "model": "distilgpt2" }
```

Response:
```json
{ "response": "Hello world, I am a language model..." }
```

---

### `POST /generate/stream`

Request:
```json
{ "prompt": "Once upon a time", "model": "distilgpt2" }
```

Response:  
Plain-text response streamed **token by token**. please Using CLI to see streamed **token by token**

curl -N -X POST http://localhost:8000/generate/stream -H "Content-Type: application/json" -d '{"prompt": "Once upon a time","model": "distilgpt2"}'


---

## ğŸ–¥ï¸ CLI Tool

### Generate once:
```bash
python3 cli.py generate "Tell me a joke about LLMs"
```

### Stream output:
```bash
python3 cli.py stream "Write a short story"
```

---

## ğŸ§ª Run Tests

```bash
pytest -v tests/test_main.py
```

---

## ğŸ§  Design Decisions & Improvements

- **True Token Streaming:** Implemented using `TextIteratorStreamer` for realistic generation.
- **Dynamic Model Loader:** Uses `AutoModelForCausalLM` and `AutoTokenizer`, with caching via `lru_cache`.
- **Rate Limiting:** Integrated `slowapi` to throttle request rate per IP.
- **SQLite Logging:** Prompts and responses stored in `interactions` table with timestamp.
- **Modular Design:** Clear separation between app logic, logging, streaming, and model handling.
- **CLI & Docker:** Simplifies testing and deployment in any environment.

---

## ğŸ’¡ Future Work

- âœ… (Done) True real-time token streaming
- âœ… (Done) Switchable models with lazy loading
- âœ… (Done) SQLite-based structured logging
- âœ… (Done) Docker containerization
- ğŸ” Add authentication for protected endpoints
- ğŸ“Š Add Prometheus/Grafana-compatible metrics exporter
- ğŸŒ Add Web UI (Gradio or Streamlit)

---

## ğŸ“‚ Project Structure

```
minivault-api/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ main.py              # FastAPI app
â”‚   â”œâ”€â”€ models.py            # HuggingFace loader + cache
â”‚   â”œâ”€â”€ streamer.py          # Streaming logic
â”‚   â”œâ”€â”€ logger.py            # SQLite logging
â”‚   â”œâ”€â”€ middlewares.py       # Rate limiter
â”‚   â””â”€â”€ schemas.py           # Request models
â”œâ”€â”€ cli.py                   # Typer CLI
â”œâ”€â”€ db/minivault.db          # SQLite DB
â”œâ”€â”€ logs/log.jsonl           # (Optional) legacy logs
â”œâ”€â”€ tests/test_api.py        # API test
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ .dockerignore
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ğŸ™Œ Acknowledgment

Built as part of the ModelVault take-home challenge.  
Fully offline. No cloud APIs are used.
